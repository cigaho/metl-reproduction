# Figure 3 - Extrapolition Skill for METL

# Explain figure 3
This figure (Figure 3) systematically evaluates the extrapolation capabilities of the METL model and other baseline methods across four distinct and challenging scenarios. The core question addressed is whether a model can make accurate predictions about protein fitness when confronted with entirely new information that was absent from its training data. Each subfigure (a, b, c, d) corresponds to a specific type of extrapolation task, testing the model's ability to generalize beyond its experience. The performance is quantified using the Spearman rank correlation coefficient, with higher values indicating better prediction accuracy. Results are presented as box plots across multiple protein datasets (e.g., Average, GFP, DLG4), allowing for a comparison of how different models (like METL, ESM-2, Rosetta) perform on the same challenge.

Figure 3a tests "Mutation-type extrapolation," examining if a model can predict the effect of a specific amino acid change (e.g., Alanine to Glycine) at a given position, even if that exact change was never seen during training. This is achieved by strictly separating mutation types into training and testing sets. Small, single-domain proteins such as GB1 and GFP have especially smooth, stability-dominated landscapes, so once the model has seen a few amino acids at each position it can interpolate to the rest. In contrast, larger multi-domain proteins (Pab1, PTENs, Ube4b) have many positions where the same site can participate in different contacts in different conformations or regulatory states, so the mapping “side-chain chemistry → fitness” is noisier and the spread between methods widens, but overall the task remains the easiest of the four because the model never has to reason about completely unseen positions or strong epistasis, only about which side chains are compatible with the existing local environment.

Panel b becomes harder because now the model must predict effects at positions where no mutations were observed in the experimental training set. Biologically this means guessing which residues are buried core residues, which sit in flexible loops, which line an active or binding site, etc., without ever seeing those positions actually change in the assay. That requires a strong prior about structure and functional hot spots. Not surprisingly, average performance drops compared with panel a, and differences between methods become clearer: ProteinNPT and METL-Local sit highest on average because their priors explicitly encode dense local mutational scans from Rosetta—“all mutations at all positions”—so they already know, before seeing experimental data, which backbone contexts tend to be tolerant or sensitive. For simple proteins like GFP and GB1, this prior is very informative, so their points are still relatively high. For Pab1, PTEN and Ube4b, with long disordered segments, multiple domains and context-dependent interaction surfaces, the same “local” prior is less predictive: a position that looks solvent-exposed in one structure may actually be an interaction interface in another state, so unseen positions are much harder to extrapolate to, and all models show lower, more scattered Spearman values.

In panel c the regime changes: the models are trained only on single mutants and then asked to predict the fitness of variants carrying multiple substitutions. This probes how mutations combine—whether the landscape is roughly additive or dominated by epistasis. Biologically, in these particular datasets the sampled region of sequence space appears to be largely additive: most mutations mainly perturb stability or binding in ways that sum approximately linearly, without many strong rescuing or synthetic-lethal interactions. That’s why all the supervised models, including even a simple linear regressor, sit very high (average Spearman >0.75). ProteinNPT is slightly worse on average, in part because some datasets (for example GFP) have more non-additive pockets where brightness depends on precise chromophore packing, and a model that leans heavily on evolutionary coupling patterns can be misled. But the overall shape of the panel—tight clusters near the top across proteins—reflects the biological reality that, at the modest mutation depths used here, most combinations behave like “sum of independent local stability changes” rather than highly epistatic allosteric rewiring.

Panel d is biologically the most demanding setting: the models see only variants with fitness below wild-type and must predict which unseen variants exceed wild-type. In most proteins, “breaking” function is easy—many random mutations destabilize the fold, disrupt interfaces, or perturb expression—while improving on an already well-adapted wild-type often requires rare, very specific substitutions that exploit subtle allosteric or kinetic effects. Training data that are almost all damaging therefore teach the model a very asymmetric landscape: it learns what a broken protein looks like, not how to climb the narrow ridge of high fitness. That is why, except for GB1, all methods have low correlations (<0.3). GB1 again stands out because its assay (binding of a small 56-aa domain to IgG Fc) is strongly stability/binding-dominated; the same biophysical priors that capture how to avoid breaking the fold also point towards mutations that slightly tighten binding, so METL-Local, METL-Global and other supervised models reach Spearman >0.7 there. In contrast, for large, multi-functional proteins like PTENs, Pab1 and Ube4b, beneficial mutations may tune regulatory motifs, partner specificity or phase behavior rather than gross stability, and those subtle “gain-of-function” mechanisms are almost invisible when the model has been trained purely on the many ways to reduce activity, so the whole panel looks much flatter and closer to noise.

# Reproducing the figures

## Figure 3a — Mutation-type extrapolation

Goal: can the model predict fitness for mutation types it has never seen?
How we did it: we parse each variant into mutation tuples (wt_aa, position, mut_aa). We then split the set of mutation types into disjoint train/test pools (e.g., 80%/20%). A variant goes to the train pool only if all of its mutation types are in the train set; it goes to test only if all are in the test set; “mixed” variants are discarded. We encode sequences once with the METL-Local TEM-1 encoder, then train a small regression head on top of the frozen backbone (phase-1), followed by a light fine-tune (phase-2). For each replicate we do a 90/10 train/val split within the train pool, pick the best epoch by val MSE, and report Spearman ρ on the mutation-type-disjoint test set; the figure plots the median (and IQR) over 9 replicates.

## Figure 3b — Position (site) extrapolation

Goal: can the model generalize to unseen sequence positions?
How we did it: from the double-mutant file we extract the unordered site indices (i, j) for each variant. We randomly split the set of positions into train vs test (e.g., 80%/20%). A variant belongs to train only if both sites lie in the train-site set; it belongs to test only if both sites lie in the test-site set; “cross” pairs are discarded. Training/inference are identical to 3a (encode once; head-only phase then a short fine-tune; 90/10 inner split; best-val checkpoint). We evaluate Spearman ρ on variants whose sites are entirely unseen during training and store the 9-replicate median.

## Figure 3c — Combination (pair) extrapolation

Goal: can the model compose known parts: i.e., predict doubles where each single-site mutation type has been seen before, but their combination has never been seen?
How we did it: we keep doubles only. We ensure each individual mutation type (e.g., A42G and L85F) appears in the training set somewhere (possibly paired with other sites), but the specific unordered pair of mutation types (or, equivalently, the exact unordered site-pair with those identities) is held out for test. Concretely: build the set of observed double combinations from the data, sample a train subset of combinations, then define the test set as doubles whose exact combination is unseen while their component mutation types are present elsewhere in train. Train as in 3a (encode once, two-phase training, inner 90/10 split) and report Spearman ρ on the unseen-combination test set, aggregating the 9-replicate median.

## Figure 3d — Order extrapolation (singles → higher order)

Goal: can the model trained only on single mutants predict multi-mutant fitness?
How we did it in principle: take all single mutants as the training set; hold out all ≥2-mutant variants for testing; train the same two-phase target on singles and compute Spearman ρ on higher-order mutants.
TEM-1 wrinkle: the provided TEM-1 TSV contains only doubles. So true order-extrapolation can’t be run on this file. For TEM-1 we instead used a double-only proxy that still measures combinatorial generalization: train on a subset of unordered site-pairs and test on disjoint site-pairs (unseen pairs). The training procedure, validation, and reporting (median over 9 replicates) stay identical to the other panels.

# About Figure 5
Fig. 5 asks whether adding task-specific, physics-based pretraining signals that reflect GB1â€™s real function (binding to IgG) can improve downstream GB1 mutation-effect prediction over a â€œvanillaâ€ Rosetta-only METL pretrain. Concretely, the paper compares a standard local METL pretrain (â€œMETL-Lâ€) against a binding-aware pretrain (â€œMETL-Bindâ€) that incorporates features computed on a GB1â€“IgG complex. The hypothesis is that if the backbone has â€œseenâ€ binding-relevant geometry/energetics during pretraining, the small supervised heads trained on DMS will need fewer labeled examples and will generalize better. Only figure5b and figure5c are content for us to reproduce, the rest are for explainations.

Fig. 5b is the learning-curve result on the GB1 dataset (x-axis: experimental train size; y-axis: Spearman on a fixed test split). The curve for METL-Bind sits consistently above METL-L, with the largest gains at low N, demonstrating better sample-efficiency when the backbone is pretrained on binding-aware signals rather than structure-only signals. This is the exact behavior our script targets: we instantiate the GB1 domain (B1, 56 aa) and evaluate Spearman across many replicate subsamples per N to trace the median and IQR, mirroring the figure.

Fig. 5c then breaks performance out by feature regimes within the GB1 landscape (e.g., grouped by mutation category/position/regime/score, as defined in the paperâ€™s methods) to show the improvement is broad-based rather than confined to one slice of the data. In other words, the binding-aware pretraining doesnâ€™t just help at, say, interface positions â€” it shifts the distribution upward across multiple GB1-relevant strata.

In our reproduction, we use the canonical GB1 B1 sequence length (56 aa) and the 2GB1 structure for PDB-aware attention, aligning with the paperâ€™s binding context for GB1; this is critical because some public GB1 sequences differ by one residue and will mis-index mutations if you donâ€™t normalize to the 56-residue B1 domain. Thatâ€™s why we validated WT length and variant positions, used the PDB key for the backboneâ€™s relative-position buckets, and then traced the same Spearman-vs-N protocol as the paper.

# Difficulties about figure 5b and 5c (not mentioned in the presentation)
The METL-bind is not provided and reimplement it alone would need the pyrosetta, which is not avaliable to us.

For 5b there was a hidden WT/variant mismatch: GB1 B1 is 56 aa, but we initially fed a 55-aa WT string, which made encode_variants hit â€œindex out of boundsâ€ when a variant referenced position 56. We fixed this by hard-setting a clean 56-aa GB1 WT and validating every mutationâ€™s 1-based position against length 56 before encoding. Second, METLâ€™s PDB-aware relative attention expects a real pdb_fn; passing None led to a TypeError: expected str â€¦ not NoneType. We resolved this by downloading 2GB1.pdb and threading pdb_fn=GB1_PDB through the backbone call in forward. (Background: GB1 B1â€™s 56-residue length is standard in the literature and PDB resources; thatâ€™s why the 55-aa WT string guaranteed failure. )

We runs the Fig. 5b learning-curve protocol on GB1 with PDB-aware pretraining  (2GB1.pdb). For each training size ğ‘ âˆˆ { 10 , 20 , â€¦ , 20480 } Nâˆˆ{10,20,â€¦,20480} it: (i) uses a fixed 10% held-out test split, (ii) samples ğ‘ N examples for train/val (80/20), (iii) trains a two-phase target model (head-only â†’ fine-tune) with AdamW, weight-decay, linear warmup + cosine decay, max-grad-norm clipping, and (iv) prints epoch-progress and then the test Spearman. For each ğ‘ N it aggregates replicates, prints median + IQR, and finally saves everything to fig5b_results.npz in your repo root. That NPZ contains two objects (when available): metl_l and metl_bind, each a dict keyed by train size with values {rhos, median, q25, q75}. (This matches the â€œfunction-specific pretraining improves GB1â€ setup described around Fig. 5 in the paper. )
